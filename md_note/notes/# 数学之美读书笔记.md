# 数学之美读书笔记  
  
## 语意模型--马尔可夫假设  
  
- 每个词的出现概率只和前一个词有关，根据大数定律，两者共同出现的概率等于语料库中两者一起出现的次数除以语料库大小。  
- n阶马尔可夫假设(n元模型）：出现概率与前面n-1个词有关  

- 解决语料库中零概率问题的方法：古德－图灵估计
  - 假定r较小时统计可能不可靠，因此出现r次的词不应有那么高的权重
  - 原本的计算概率方式：
    - $N=\displaystyle\sum^{\infty}_{r=1}rN_r$
    - 其中出现r次的词有$Nr$个,N为语料库大小
  - 现在的方式：
    - $d_r=(r+1)\cdot N_{r+1}/N_r$
    - 其中$d_r$是估计的概率

- 局限：  
  - 复杂度大  
  - 难找到上下文关联  
  
## 隐含马尔科夫模型  
  
- 马尔科夫链
  - 一个状态本该和以前所有状态有关，但是马尔科夫链中只考虑上**一个**状态
  - 实质是一个有穷状态机，随机选择一个状态开始，按自身规则随机转移到下一个状态
  - 组成成分：
    - 状态
    - 状态转移概率
    - 结果
    - 产生结果概率（独立输出假设）

- 隐含马尔科夫模型：缺少上述成分中的一个
  - 三个基本问题
    1. 给定模型，计算产生某个特定输出的概率: forward-backward算法
    2. 给定模型和输出序列，找到最可能的状态序列: 维特比算法
    3. 给定足够观测数据，估计模型的各个参数（如下）

  - 需要求解
    1. $P(o_t|s_t) = \frac{P(o_t,s_t)}{P(s_t)}$--------（1）
    2. $P(s_t|s_{t-1}) = \frac{P(s_{t-1},s_t)}{P(s_{t-1})}$--------（2）  
其中$s_t$为第t个状态，$o_t$为第t个输出  

  - 有监督：如果知道$s_t$出现次数，每次经过这个状态时产生的输出，那么$P(o_t|s_t) = \frac{\#(o_t,s_t)}{\#(s_t)}$

  - 无监督：使用鲍姆-韦尔奇算法：
    1. 找到一组能够产生输出序列O的参数，这个初始模型记为$M_{\theta_0}$
    2. 为了找到更好的模型，我们假定已经解决了问题1，2，可以算出**这个模型产生O的概率**$P(O|M_{\theta_0})$,以及**这个模型产生O的所有途径及其概率**
    3. 这些可能路径可以看作***标记的训练数据***，根据（1）（2）式迭代至收敛
  
## 信息的度量
  
- 信息熵  
![信息熵](images/XinXiShang.jpg)  
  - 定义：$H(X)=-\displaystyle\sum_{x\in X}P(x)logP(x)$
  - 冗余度：文件大小（BIT）与文件所含信息熵的差值（汉语冗余度较小）
  
- 信息的作用：消除不确定性
  - 在Y的条件下X的**条件熵**：$H(X|Y)=-\displaystyle\sum_{x\in X,y\in Y}P(x,y)logP(x|y)<H(X)$--------(1) 稍后证明
  TODO
  
- 互信息：对**两个随机事件相关性**的度量
  - $I(X;Y)=-\displaystyle\sum_{x\in X,y\in Y}log{\frac{P(x,y)}{P(x)\cdot P(y)}}$
  - $I(X;Y)== H(X)-H(X|Y)$

- 相对熵（交叉熵，Kullback-Leibler Divergence，KL）:衡量两个取值为正数的函数的相似性
  - $KL(f(x)||g(x))=\displaystyle\sum_{x\in X}f(x)\cdot log{\frac{f(x)}{g(x)}}$
  - 结论
    1. 函数相差越大，相对熵越大
    2. 对于概率分布/概率密度函数，如果取值大于零，相对熵可以度量两个随机分布的差异
  
## 搜索引擎
